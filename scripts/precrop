#!/usr/bin/env python
"""Crop preprocessed dataset and split into training/val parts"""
# pylint: disable=missing-function-docstring

import argparse
import os
import multiprocessing
import tqdm

import numpy  as np
import pandas as pd

from toytools.collect import train_val_test_split, load_image
from toytools.consts  import (
    DIR_FAKE, DIR_REAL, SPLIT_TRAIN, SPLIT_VAL, SPLIT_TEST
)
from toytools.transform import crop_image

def parse_cmdargs():
    parser = argparse.ArgumentParser("Precompute crops for toyzero dataset")

    parser.add_argument(
        'path',
        help    = 'Path to the preprocessed dataset',
        metavar = 'PATH',
        type    = str,
    )

    parser.add_argument(
        'outdir',
        help    = 'Directory where to save cropped dataset',
        metavar = 'OUTDIR',
        type    = str,
    )

    parser.add_argument(
        '--shuffle',
        action  = 'store_true',
        dest    = 'shuffle',
        help    = 'Whether to shuffle data',
    )

    parser.add_argument(
        '-s', '--seed',
        default = 0,
        dest    = 'seed',
        help    = 'Seed of a pseudo random generator',
        type    = int,
    )

    parser.add_argument(
        '--val-size',
        default = 0.2,
        dest    = 'val_size',
        help    = (
            'Size of the validation dataset. '
            'If val-size <= 1, then it is interpreted as a fraction'
        ),
        type    = float,
    )

    parser.add_argument(
        '--test-size',
        default = 0,
        dest    = 'test_size',
        help    = (
            'Size of the test dataset. '
            'If test-size <= 1, then it is interpreted as a fraction'
        ),
        type    = float,
    )

    return parser.parse_args()

class CroppingExtractorWorker:
    # pylint: disable=missing-class-docstring

    def __init__(self, data_root, savedir_fake, savedir_real):
        self._data_root    = data_root
        self._savedir_fake = savedir_fake
        self._savedir_real = savedir_real

    # pylint: disable=no-self-use
    def _extract_crop(self, image, sample, savedir):
        crop_region = (sample.x, sample.y, sample.width, sample.height)

        crop = crop_image(image, crop_region)
        crop = (crop - sample.bkg)

        basename, ext = os.path.splitext(sample.image)
        fname = '%s_%dx%d%s' % (basename, sample.x, sample.y, ext)
        path  = os.path.join(savedir, fname)

        np.savez_compressed(path, crop)

    def __call__(self, sample):
        image_fake = load_image(self._data_root, True,  sample.image)
        image_real = load_image(self._data_root, False, sample.image)

        self._extract_crop(image_fake, sample, self._savedir_fake)
        self._extract_crop(image_real, sample, self._savedir_real)

def load_dataset(path):
    return pd.read_csv(path, index_col = 'index')

def split_into_parts(df, val_size, test_size, shuffle, seed):
    prg = None

    if shuffle:
        prg = np.random.default_rng(seed)

    train_indices, val_indices, test_indices = train_val_test_split(
        len(df), val_size, test_size, shuffle, prg
    )

    return (
        df.iloc[train_indices], df.iloc[val_indices], df.iloc[test_indices]
    )

def extract_crops(data_root, df, savedir_fake, savedir_real):
    os.makedirs(savedir_fake, exist_ok = True)
    os.makedirs(savedir_real, exist_ok = True)

    progbar = tqdm.tqdm(
        desc = 'Extracting Crops', total = len(df), dynamic_ncols = True
    )

    worker = CroppingExtractorWorker(data_root, savedir_fake, savedir_real)

    with multiprocessing.Pool() as pool:
        for _ in pool.imap_unordered(worker, (x[1] for x in df.iterrows())):
            progbar.update()

    progbar.close()

def extract(data_root, df_train, df_val, df_test, outdir):
    if len(df_train) > 0:
        print("Extracting Training Crops")
        dir_fake = os.path.join(outdir, SPLIT_TRAIN, DIR_FAKE)
        dir_real = os.path.join(outdir, SPLIT_TRAIN, DIR_REAL)
        extract_crops(data_root, df_train, dir_fake, dir_real)

    if len(df_val):
        print("Extracting Validation Crops")
        dir_fake = os.path.join(outdir, SPLIT_VAL, DIR_FAKE)
        dir_real = os.path.join(outdir, SPLIT_VAL, DIR_REAL)
        extract_crops(data_root, df_val, dir_fake, dir_real)

    if len(df_test):
        print("Extracting Test Crops")
        dir_fake = os.path.join(outdir, SPLIT_TEST, DIR_FAKE)
        dir_real = os.path.join(outdir, SPLIT_TEST, DIR_REAL)
        extract_crops(data_root, df_test, dir_fake, dir_real)

def main():
    cmdargs = parse_cmdargs()

    df = load_dataset(cmdargs.path)

    df_train, df_val, df_test = split_into_parts(
        df, cmdargs.val_size, cmdargs.test_size, cmdargs.shuffle, cmdargs.seed
    )

    data_root = os.path.dirname(cmdargs.path)
    extract(data_root, df_train, df_val, df_test, cmdargs.outdir)

if __name__ == '__main__':
    main()

